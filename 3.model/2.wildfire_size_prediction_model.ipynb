{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7673b-5322-440a-8b0f-f92b7cda79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae98f30-f975-4e17-8ff8-adc2af1abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel(\"fp-historical-wildfire-data-2006-2023.xlsx\")\n",
    "\n",
    "# Create a mapping table for forest_area\n",
    "mapping = {\n",
    "    'C': 'Calgary',\n",
    "    'E': 'Edson',\n",
    "    'G': 'Grande Prairie',\n",
    "    'H': 'High Level',\n",
    "    'L': 'Lac La Biche',\n",
    "    'M': 'Fort McMurray',\n",
    "    'P': 'Peace River',\n",
    "    'R': 'Rocky Mountain House',\n",
    "    'S': 'Slave Lake',\n",
    "    'W': 'Whitecourt'\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "mapping_df = pd.DataFrame(list(mapping.items()), columns=['initial', 'forest_area_full'])\n",
    "\n",
    "# Extract the first letter of fire_number to create forest_area\n",
    "data['forest_area'] = data['fire_number'].str[0]\n",
    "\n",
    "# Merge the original data with the mapping DataFrame\n",
    "data = data.merge(mapping_df, left_on='forest_area', right_on='initial', how='left')\n",
    "\n",
    "# Replace the forest_area with the full name\n",
    "data['forest_area'] = data['forest_area_full']\n",
    "\n",
    "# Drop the temporary columns used for merging\n",
    "data.drop(columns=['initial', 'forest_area_full'], inplace=True)\n",
    "\n",
    "# Data preprocessing\n",
    "data['reported_date'] = pd.to_datetime(data['reported_date'], errors='coerce')\n",
    "data['reported_month'] = data['reported_date'].dt.month\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = [\n",
    "    'reported_month', \n",
    "    'forest_area', \n",
    "    'fire_location_latitude', 'fire_location_longitude',\n",
    "    'general_cause_desc', \n",
    "    'weather_conditions_over_fire', \n",
    "    'temperature', 'relative_humidity', 'wind_direction',\n",
    "    'wind_speed', 'size_class'\n",
    "]\n",
    "\n",
    "data_relevant = data[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903ca02-e35e-42c6-8d0e-7e3608f8126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reported_month                  0\n",
      "forest_area                     0\n",
      "fire_location_latitude          0\n",
      "fire_location_longitude         0\n",
      "general_cause_desc              0\n",
      "weather_conditions_over_fire    0\n",
      "temperature                     0\n",
      "relative_humidity               0\n",
      "wind_direction                  0\n",
      "wind_speed                      0\n",
      "size_class                      0\n",
      "forest_cause_combined           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Data cleansing\n",
    "numerical_columns = data_relevant.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# numerical_columns .remove('fire_location_latitude')\n",
    "categorical_columns = data_relevant.select_dtypes(include=[object]).columns.tolist()\n",
    "# categorical_columns.remove('size_class')\n",
    "\n",
    "# Fill null value by median for numerical_columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_relevant[numerical_columns] = imputer.fit_transform(data_relevant[numerical_columns])\n",
    "# fill null value by using \"missing\" for categorical_columns\n",
    "data_relevant[categorical_columns] = data_relevant[categorical_columns].fillna('missing')\n",
    "\n",
    "# Create a feature combination\n",
    "data_relevant['forest_cause_combined'] = data_relevant['forest_area'] + '_' + data_relevant['general_cause_desc']\n",
    "\n",
    "# Append to categorical_columns\n",
    "categorical_columns.append('forest_cause_combined')\n",
    "\n",
    "# check the result\n",
    "print(data_relevant.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f61c3e-1246-405d-a34b-da3f9c06602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Encoding categorical_columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data_relevant[column] = data_relevant[column].astype(str)\n",
    "    le.fit(list(data_relevant[column]) + ['unknown'])  \n",
    "    data_relevant[column] = data_relevant[column].apply(lambda x: x if x in le.classes_ else 'unknown')\n",
    "    data_relevant[column] = le.transform(data_relevant[column])\n",
    "    label_encoders[column] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdff179-2813-47ea-b4b7-50fcb498e271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   reported_month  forest_area  fire_location_latitude  \\\n",
      "0        -1.30896            6                0.484101   \n",
      "1        -1.30896            1               -0.526989   \n",
      "2        -1.30896            1               -0.525242   \n",
      "3        -1.30896            1               -0.526033   \n",
      "4        -1.30896            6                0.484101   \n",
      "\n",
      "   fire_location_longitude  general_cause_desc  weather_conditions_over_fire  \\\n",
      "0                -0.843053                  11                             2   \n",
      "1                -0.299861                   3                             2   \n",
      "2                -0.161957                   3                             2   \n",
      "3                -0.168477                   3                             2   \n",
      "4                -0.786551                   6                             2   \n",
      "\n",
      "   temperature  relative_humidity  wind_direction  wind_speed  size_class  \\\n",
      "0     0.002880          -1.965742               9   -0.813263           0   \n",
      "1    -0.831584          -1.287494               9    0.187825           1   \n",
      "2    -0.831584          -1.287494               9    0.187825           1   \n",
      "3    -0.831584          -1.287494               9    0.187825           0   \n",
      "4    -1.666047          -0.439683               9   -0.813263           0   \n",
      "\n",
      "   forest_cause_combined  \n",
      "0                     96  \n",
      "1                     18  \n",
      "2                     18  \n",
      "3                     18  \n",
      "4                     91  \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# numerical_columns standration \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_relevant[numerical_columns] = scaler.fit_transform(data_relevant[numerical_columns])\n",
    "# check\n",
    "print(data_relevant.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f36d9e-1150-443c-8162-86806a92a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Separation of features and target variables\n",
    "# X = data_relevant.drop(columns=['size_class'])\n",
    "X = data_relevant.drop(columns=['size_class','general_cause_desc','forest_area'])\n",
    "y = data_relevant['size_class']\n",
    "\n",
    "# Split to training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c4661-78a4-46a6-a221-ddbfc01f7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=2024)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897c484-08ef-47b2-be94-47f6cc36539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1688\n",
      "[LightGBM] [Info] Number of data points in the train set: 67180, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.79      3365\n",
      "           1       0.44      0.30      0.35      1282\n",
      "           2       0.16      0.13      0.14       261\n",
      "           3       0.07      0.19      0.10        69\n",
      "           4       0.16      0.38      0.22        88\n",
      "\n",
      "    accuracy                           0.63      5065\n",
      "   macro avg       0.32      0.36      0.32      5065\n",
      "weighted avg       0.63      0.63      0.63      5065\n",
      "\n",
      "Feature importances:\n",
      "1. Feature wind_speed (1076)\n",
      "2. Feature fire_location_latitude (1045)\n",
      "3. Feature forest_cause_combined (988)\n",
      "4. Feature fire_location_longitude (957)\n",
      "5. Feature reported_month (950)\n",
      "6. Feature relative_humidity (830)\n",
      "7. Feature temperature (737)\n",
      "8. Feature wind_direction (587)\n",
      "9. Feature weather_conditions_over_fire (330)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "# Define the LGBM model with best parameters found\n",
    "# 'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.8\n",
    "lgbm_model = LGBMClassifier(\n",
    "    colsample_bytree=1.0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_estimators=50,\n",
    "    subsample=0.8,\n",
    "    force_col_wise='true',\n",
    "    random_state=2024\n",
    ")\n",
    "# Train the model\n",
    "lgbm_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the feature importances\n",
    "importances = lgbm_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature importances:\")\n",
    "for i in range(len(importances)):\n",
    "    print(f\"{i + 1}. Feature {X.columns[indices[i]]} ({importances[indices[i]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39478c7f-4694-4fbd-9e05-5d3e2cf3377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.79      3365\n",
      "           1       0.42      0.32      0.36      1282\n",
      "           2       0.15      0.14      0.14       261\n",
      "           3       0.08      0.17      0.11        69\n",
      "           4       0.18      0.35      0.24        88\n",
      "\n",
      "    accuracy                           0.63      5065\n",
      "   macro avg       0.32      0.36      0.33      5065\n",
      "weighted avg       0.63      0.63      0.63      5065\n",
      "\n",
      "Feature importances:\n",
      "1. Feature wind_speed (0.16743145883083344)\n",
      "2. Feature weather_conditions_over_fire (0.15818674862384796)\n",
      "3. Feature reported_month (0.12257900089025497)\n",
      "4. Feature fire_location_latitude (0.11092520505189896)\n",
      "5. Feature forest_cause_combined (0.09450942277908325)\n",
      "6. Feature temperature (0.09324748069047928)\n",
      "7. Feature wind_direction (0.09277991205453873)\n",
      "8. Feature relative_humidity (0.08365174382925034)\n",
      "9. Feature fire_location_longitude (0.07668904960155487)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# Define the xgboost model with best parameters found\n",
    "# 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.9\n",
    "xgboost_model = XGBClassifier(\n",
    "    colsample_bytree=0.9,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    n_estimators=100,\n",
    "    subsample=0.9,\n",
    "    random_state=2024\n",
    ")\n",
    "# Train the model\n",
    "xgboost_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the feature importances\n",
    "importances = xgboost_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature importances:\")\n",
    "for i in range(len(importances)):\n",
    "    print(f\"{i + 1}. Feature {X.columns[indices[i]]} ({importances[indices[i]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba3f3c-ad2c-4063-a749-2f7e88635031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoders.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(lgbm_model, 'lgbm_model.pkl')\n",
    "joblib.dump(xgboost_model, 'xgboost_model.pkl')\n",
    "\n",
    "# Save the scalers and encoders\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(imputer, 'imputer.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
